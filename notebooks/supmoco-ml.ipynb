{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96befd0-e2e8-4f5d-8262-f7aed49a2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b534d002-9124-4b62-84f4-40f6ec3aab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import rich\n",
    "import numpy as np\n",
    "import pickle\n",
    "import wandb\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from configs.finetune import FinetuneConfig\n",
    "from tasks.classification import Classification\n",
    "\n",
    "from models.backbone.base import calculate_out_features\n",
    "from models.backbone.densenet import DenseNetBackbone\n",
    "from models.backbone.resnet import build_resnet_backbone\n",
    "from models.head.projector import MLPHead\n",
    "\n",
    "from datasets.brain import BrainProcessor, Brain, BrainMoCo\n",
    "from datasets.transforms import make_transforms, compute_statistics\n",
    "\n",
    "from utils.logging import get_rich_logger\n",
    "from utils.gpu import set_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1abd6a-c709-41b7-8fe9-fc46d184fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.metrics import classification_result\n",
    "\n",
    "import collections\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b4ee22-e6c4-42c9-9c50-b8e170358155",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(data_loader, backbone, projector, device, use_projector=True):\n",
    "\n",
    "    backbone.to(device)\n",
    "    backbone.eval()\n",
    "    projector.to(device)\n",
    "    projector.eval()\n",
    "\n",
    "    reprs, labels = [], []\n",
    "    for batch in tqdm.tqdm(data_loader):\n",
    "        if use_projector:\n",
    "            z = projector(backbone(batch['x'].to(device, non_blocking=True)))        \n",
    "            reprs += [F.normalize(z, dim=1)]\n",
    "        else:\n",
    "            z = backbone(batch['x'].to(device, non_blocking=True))\n",
    "            z = nn.Flatten()(nn.AdaptiveAvgPool3d(1)(z))\n",
    "            reprs += [z]\n",
    "        labels += [batch['y'].to(device)]\n",
    "    \n",
    "    reprs = torch.cat(reprs, dim=0).cpu().numpy()\n",
    "    labels = torch.cat(labels, dim=0).cpu().numpy()\n",
    "\n",
    "    backbone.to('cpu')\n",
    "    projector.to('cpu')\n",
    "\n",
    "    return reprs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da121795-ff71-43c1-8a10-2d156413ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashs = [\"2022-07-02_18-54-27\", \"2022-07-02_18-53-46\", \"2022-07-02_17-15-34\",\n",
    "         \"2022-07-02_17-15-14\", \"2022-07-02_11-20-21\", \"2022-07-02_11-17-38\",\n",
    "         \"2022-07-02_09-40-42\", \"2022-07-02_09-38-52\", \"2022-07-02_08-00-57\", \"2022-07-02_08-00-31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ed088b-06e3-4bd2-be59-98f254c235ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cff74c-61fe-49f5-87da-c4615b8bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:42<00:00,  1.23s/it]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n",
      "100%|██████████| 35/35 [00:38<00:00,  1.09s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n",
      "100%|██████████| 36/36 [00:39<00:00,  1.09s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n",
      "100%|██████████| 36/36 [00:36<00:00,  1.02s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n",
      "100%|██████████| 36/36 [00:38<00:00,  1.06s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.18s/it]\n",
      "100%|██████████| 36/36 [00:37<00:00,  1.03s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n",
      "100%|██████████| 35/35 [00:35<00:00,  1.02s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.02it/s]\n",
      "100%|██████████| 35/35 [00:35<00:00,  1.02s/it]\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.02s/it]\n",
      "100%|██████████| 35/35 [00:37<00:00,  1.09s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\n",
      "100%|██████████| 35/35 [00:38<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.14it/s]\n",
      "100%|██████████| 35/35 [00:38<00:00,  1.10s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.05it/s]\n",
      "100%|██████████| 35/35 [00:37<00:00,  1.08s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n",
      "100%|██████████| 36/36 [00:38<00:00,  1.07s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n",
      "100%|██████████| 36/36 [00:38<00:00,  1.08s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n",
      "100%|██████████| 36/36 [00:38<00:00,  1.08s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n",
      "100%|██████████| 36/36 [00:30<00:00,  1.17it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "100%|██████████| 35/35 [00:35<00:00,  1.02s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.11it/s]\n",
      "100%|██████████| 35/35 [00:37<00:00,  1.08s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.02it/s]\n",
      "100%|██████████| 36/36 [00:38<00:00,  1.07s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.04it/s]\n",
      "100%|██████████| 36/36 [00:38<00:00,  1.07s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, hash in enumerate(hashs):\n",
    "    \n",
    "    print('--------------------------------- ', i, ' ---------------------------------')\n",
    "    \n",
    "    # set config\n",
    "    config = edict()\n",
    "    config.pretrained_file_prefix = '../checkpoints/pet-supmoco/resnet/'\n",
    "    config.gpus = ['0']\n",
    "    config.server = 'dgx'\n",
    "    local_rank = 0\n",
    "    \n",
    "    # load pretrained model's config\n",
    "    config.pretrained_dir = f'{config.pretrained_file_prefix}{hash}'\n",
    "    config.pretrained_file = os.path.join(config.pretrained_dir, \"ckpt.last.pth.tar\")\n",
    "\n",
    "    pretrained_config = os.path.join(config.pretrained_dir, \"configs.json\")\n",
    "    with open(pretrained_config, 'rb') as fb:\n",
    "        pretrained_config = json.load(fb)\n",
    "\n",
    "    pretrained_config_names = [\n",
    "        # data_parser\n",
    "        'data_type', 'root', 'data_info', 'mci_only', 'n_splits', 'n_cv',\n",
    "        'image_size', 'small_kernel', 'random_state',\n",
    "        'intensity', 'crop', 'crop_size', 'rotate', 'flip', 'affine', 'blur', 'blur_std', 'prob',\n",
    "        # model_parser\n",
    "        'backbone_type', 'init_features', 'growth_rate', 'block_config', 'bn_size', 'dropout_rate',\n",
    "        'arch', 'no_max_pool',\n",
    "        # train\n",
    "        'batch_size',\n",
    "        # moco / supmoco\n",
    "        'alphas',\n",
    "        # others\n",
    "        'task', 'projector_dim'\n",
    "    ]\n",
    "\n",
    "    for name in pretrained_config_names:\n",
    "        if name in pretrained_config.keys():\n",
    "            setattr(config, name, pretrained_config[name])\n",
    "    \n",
    "    set_gpu(config)\n",
    "    np.random.seed(config.random_state)\n",
    "    torch.manual_seed(config.random_state)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    # Networks\n",
    "    if config.backbone_type == 'densenet':\n",
    "        backbone = DenseNetBackbone(in_channels=1,\n",
    "                                    init_features=config.init_features,\n",
    "                                    growth_rate=config.growth_rate,\n",
    "                                    block_config=config.block_config,\n",
    "                                    bn_size=config.bn_size,\n",
    "                                    dropout_rate=config.dropout_rate,\n",
    "                                    semi=False)\n",
    "        activation = True\n",
    "    elif config.backbone_type == 'resnet':\n",
    "        backbone = build_resnet_backbone(arch=config.arch,\n",
    "                                         no_max_pool=config.no_max_pool,\n",
    "                                         in_channels=1,\n",
    "                                         semi=False)\n",
    "        activation = False\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if config.small_kernel:\n",
    "        backbone._fix_first_conv()\n",
    "\n",
    "    if config.crop:\n",
    "        out_dim = calculate_out_features(backbone=backbone, in_channels=1, image_size=config.crop_size)\n",
    "    else:\n",
    "        out_dim = calculate_out_features(backbone=backbone, in_channels=1, image_size=config.image_size)\n",
    "    projector = MLPHead(out_dim, config.projector_dim)    \n",
    "\n",
    "    backbone.load_weights_from_checkpoint(path=config.pretrained_file, key='backbone')\n",
    "    projector.load_weights_from_checkpoint(path=config.pretrained_file, key='head')\n",
    "    \n",
    "    # load finetune data\n",
    "    data_processor = BrainProcessor(root=config.root,\n",
    "                                    data_info=config.data_info,\n",
    "                                    data_type=config.data_type,\n",
    "                                    mci_only=config.mci_only,\n",
    "                                    random_state=config.random_state)\n",
    "    datasets = data_processor.process(n_splits=config.n_splits, n_cv=config.n_cv)\n",
    "\n",
    "    # intensity normalization\n",
    "    assert config.intensity in [None, 'scale', 'minmax']\n",
    "    mean_std, min_max = (None, None), (None, None)\n",
    "    if config.intensity is None:\n",
    "        pass\n",
    "    elif config.intensity == 'scale':\n",
    "        pass\n",
    "    elif config.intensity == 'minmax':\n",
    "        with open(os.path.join(config.root, 'labels/minmax.pkl'), 'rb') as fb:\n",
    "            minmax_stats = pickle.load(fb)\n",
    "            min_max = (minmax_stats[config.data_type]['min'], minmax_stats[config.data_type]['max'])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    train_transform, test_transform = make_transforms(image_size=config.image_size,\n",
    "                                                      intensity=config.intensity,\n",
    "                                                      mean_std=mean_std,\n",
    "                                                      min_max=min_max,\n",
    "                                                      crop=config.crop,\n",
    "                                                      crop_size=config.crop_size,\n",
    "                                                      rotate=config.rotate,\n",
    "                                                      flip=config.flip,\n",
    "                                                      affine=config.affine,\n",
    "                                                      blur=config.blur,\n",
    "                                                      blur_std=config.blur_std,\n",
    "                                                      prob=config.prob)\n",
    "\n",
    "    train_set = Brain(dataset=datasets['train'], data_type=config.data_type, transform=test_transform)\n",
    "    test_set = Brain(dataset=datasets['test'], data_type=config.data_type, transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=16, drop_last=False)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=16, drop_last=False)\n",
    "    \n",
    "    for use_projector in [True, False]:\n",
    "        train_reprs, train_labels = extract_features(train_loader, backbone, projector, local_rank, use_projector)\n",
    "        test_reprs, test_labels = extract_features(test_loader, backbone, projector, local_rank, use_projector)\n",
    "        \n",
    "        #\n",
    "        model = GradientBoostingClassifier(n_estimators=100)\n",
    "        model.fit(train_reprs, train_labels)\n",
    "        y_pred = model.predict_proba(test_reprs)\n",
    "        result = classification_result(torch.tensor(test_labels), torch.tensor(y_pred))\n",
    "        \n",
    "        result['projector'] = use_projector\n",
    "        result['classifier'] = 'gbm'\n",
    "        result['n_estimators'] = 100\n",
    "        result['random_state'] = config.random_state\n",
    "        \n",
    "        result_list.append(result)\n",
    "        del result\n",
    "        \n",
    "        #\n",
    "        model = GradientBoostingClassifier(n_estimators=10)\n",
    "        model.fit(train_reprs, train_labels)\n",
    "        y_pred = model.predict_proba(test_reprs)\n",
    "        result = classification_result(torch.tensor(test_labels), torch.tensor(y_pred))\n",
    "        \n",
    "        result['projector'] = use_projector\n",
    "        result['classifier'] = 'gbm'\n",
    "        result['n_estimators'] = 10\n",
    "        result['random_state'] = config.random_state\n",
    "        \n",
    "        result_list.append(result)\n",
    "        del result\n",
    "        \n",
    "        #\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        model.fit(train_reprs, train_labels)\n",
    "        y_pred = model.predict_proba(test_reprs)\n",
    "        result = classification_result(torch.tensor(test_labels), torch.tensor(y_pred))\n",
    "        \n",
    "        result['projector'] = use_projector\n",
    "        result['classifier'] = 'rf'\n",
    "        result['n_estimators'] = 100\n",
    "        result['random_state'] = config.random_state\n",
    "        \n",
    "        result_list.append(result)\n",
    "        del result\n",
    "        \n",
    "        #\n",
    "        model = RandomForestClassifier(n_estimators=10)\n",
    "        model.fit(train_reprs, train_labels)\n",
    "        y_pred = model.predict_proba(test_reprs)\n",
    "        result = classification_result(torch.tensor(test_labels), torch.tensor(y_pred))\n",
    "        \n",
    "        result['projector'] = use_projector\n",
    "        result['classifier'] = 'rf'\n",
    "        result['n_estimators'] = 10\n",
    "        result['random_state'] = config.random_state\n",
    "        \n",
    "        result_list.append(result)\n",
    "        del result\n",
    "        \n",
    "        #\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(train_reprs, train_labels)\n",
    "        y_pred = model.predict_proba(test_reprs)\n",
    "        result = classification_result(torch.tensor(test_labels), torch.tensor(y_pred))\n",
    "        \n",
    "        result['projector'] = use_projector\n",
    "        result['classifier'] = 'tree'\n",
    "        result['n_estimators'] = 'none'\n",
    "        result['random_state'] = config.random_state\n",
    "        \n",
    "        result_list.append(result)\n",
    "        del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeef9b01-9e01-44b8-85c0-c2abe2e5de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../ml_result.pkl', 'wb') as fb:\n",
    "    pickle.dump(result_list, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e1c56d9-7cc1-4346-a699-bab771e107bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(sum(datasets['train']['y'] == 1))\n",
    "print(sum(datasets['test']['y'] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b4c0d77-050f-4503-b4ea-418bcc5ccc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(sum(datasets['train']['y'] == 0))\n",
    "print(sum(datasets['test']['y'] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71f1806c-c418-4f1c-ac86-61ddc11d0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets['u_train']['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b7ab9-0d57-4ffe-8ffe-b84550a6e155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
